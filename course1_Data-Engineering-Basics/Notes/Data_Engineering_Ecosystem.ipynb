{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPz7b2hqBYFQdnjmV96Mbc4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# The Data Engineering Ecosystem\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AJo-sJuexvAX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A data engineer’s ecosystem consists of infrastructure, tools, frameworks, and processes for managing the flow of data. This includes:\n",
        "\n",
        "* **Data Extraction & Storage**: Collecting data from various sources (databases, APIs, web services, etc.) and storing it in appropriate repositories.\n",
        "\n",
        "* **Data Categorization**: Data is classified as structured (organized in databases), semi-structured (emails, JSON), or unstructured (images, videos, social media content).\n",
        "\n",
        "* **Data Repositories**:\n",
        "  *  **Transactional (OLTP)** – Handles real-time, high-volume operational data (e.g., banking, airline bookings).\n",
        "  \n",
        "  * **Analytical (OLAP)** – Designed for complex analysis (e.g., data warehouses, data lakes).\n",
        "* **Data Integration & Pipelines**: Data is processed, cleansed, and transformed using Extract-Transform-Load (ETL) or Extract-Load-Transform (ELT) processes.\n",
        "\n",
        "* **Programming & Querying**: SQL for querying, Python for application development, and shell scripting for automation.\n",
        "\n",
        "* **BI & Reporting Tools**: These tools, used mainly by analysts, help visualize and present data via dashboards, but they are managed by data engineers.\n",
        "\n",
        "* **Automation & Optimization**: Various frameworks and processes streamline data workflows, making data more accessible and efficient for business use.\n",
        "\n",
        "Overall, a data engineer’s ecosystem is diverse and complex, supporting the seamless movement, transformation, and analysis of data."
      ],
      "metadata": {
        "id": "SFGnneReIgXO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Tools\n",
        "\n",
        "Data is unorganized information that gains meaning through processing. It can be categorized into three types based on its structure:\n",
        "\n",
        "* **Structured Data**: Highly organized, follows a defined schema, and is stored in relational databases (SQL). Examples include spreadsheets, online forms, GPS data, and transaction logs. It can be easily analyzed using standard tools.\n",
        "\n",
        "* **Semi-Structured Data**: Has some organization but lacks a fixed schema. It includes metadata and hierarchical structures like XML, JSON, emails, and zipped files. It is often used for data integration and exchange.\n",
        "\n",
        "* **Unstructured Data**: Lacks a clear format and cannot be stored in traditional databases. Examples include social media feeds, images, videos, PDFs, and web pages. It is often stored in NoSQL databases for analysis.\n",
        "\n",
        "In summary, structured data is highly organized, semi-structured data relies on metadata, and unstructured data is freeform and complex."
      ],
      "metadata": {
        "id": "IYMZwznNJr-h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## File Formats\n",
        "As a data professional, understanding different file formats helps in choosing the best option for data storage and processing. Common formats include:\n",
        "\n",
        "* **Delimited Text Files (CSV, TSV)**: Store data in plain text with values separated by delimiters (e.g., commas, tabs). They are widely supported and flexible.\n",
        "\n",
        "* **XLSX (Excel Open XML Spreadsheet)**: A structured spreadsheet format that supports multiple worksheets and is secure since it cannot store malicious code.\n",
        "\n",
        "* **XML (Extensible Markup Language)**: A markup language for encoding data, readable by both humans and machines. It facilitates platform-independent data sharing.\n",
        "\n",
        "* **PDF (Portable Document Format)**: Developed by Adobe, it ensures documents appear the same across devices and is commonly used for legal and financial documents.\n",
        "\n",
        "* **JSON (JavaScript Object Notation)**: A widely used, language-independent format for transmitting structured data, especially in APIs and web services.\n",
        "\n",
        "Each format has its own strengths and limitations, making it crucial to select the right one based on data structure and performance needs."
      ],
      "metadata": {
        "id": "JDY7TKdIN1Pv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Sources\n",
        "\n",
        "Data professionals often work with a variety of data sources, including relational databases, NoSQL databases, big data repositories, and streaming data. While relational databases remain widely used due to their flexibility and durability, newer data types—such as logs, JSON, and XML—have driven the adoption of NoSQL databases like Cassandra and HBase, which are better suited for high-write applications like IoT and social media.\n",
        "\n",
        "Data sources today are more dynamic and diverse than ever. Common sources include:\n",
        "\n",
        "* **Relational Databases (SQL Server, MySQL, Oracle, IBM DB2)**: Used by organizations for structured data storage in applications like customer transactions, HR, and workflows.\n",
        "\n",
        "* **Flat Files & XML Datasets**: Flat files (CSV, spreadsheets) store data in tabular formats, while XML supports hierarchical structures, often used for surveys and bank statements.\n",
        "\n",
        "* **APIs & Web Services**: Enable data access from sources like social media (Twitter, Facebook), stock markets, and validation services. APIs return data in formats like JSON, XML, or plain text.\n",
        "\n",
        "* **Web Scraping**: Extracts data from websites for price comparisons, sales leads, forum data, and machine learning datasets. Popular tools include BeautifulSoup, Scrapy, and Selenium.\n",
        "\n",
        "* **Data Streams & Feeds**: Real-time data flows from IoT devices, GPS, stock tickers, retail transactions, and social media. Processing tools include Apache Kafka, Spark Streaming, and Storm.\n",
        "\n",
        "* **RSS Feeds**: Capture continuously updated data from news sites and forums for real-time tracking.\n",
        "\n",
        "These sources provide valuable data for analytics, decision-making, and business intelligence.\n",
        "\n",
        "\n",
        "#### Working with varied data formats presents unique challenges:\n",
        "\n",
        "* **Log data** is unstructured and often requires custom tools for parsing.\n",
        "\n",
        "* **XML** was widely used but is resource-intensive due to its verbose tags. JSON became the preferred format for RESTful APIs, reducing memory usage.\n",
        "\n",
        "* **Apache Avro** is gaining popularity for its efficient data storage and serialization.\n",
        "\n",
        "* **Character encoding and delimiters** can cause issues when migrating data between databases. Finding suitable delimiters in datasets containing special characters can be difficult, requiring adaptable parsing strategies."
      ],
      "metadata": {
        "id": "NYSPr-V3QvET"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Languages for Data Professionals\n",
        "\n",
        "Languages essential for data professionals can be categorized into **query languages**, **programming languages**, and **shell scripting**.\n",
        "\n",
        "* **SQL**: A query language primarily used for relational databases, enabling data retrieval, updates, and stored procedures. It is widely adopted due to its portability, simple syntax, and efficiency in handling large datasets. However, challenges arise when working across multiple database vendors, versions, and performance constraints. While moving data once may be straightforward, maintaining efficient, continuous data transfers requires flexibility and problem-solving.\n",
        "\n",
        "* **Python**: A high-level, open-source programming language known for its readability and versatility. It offers extensive libraries for data analysis (NumPy, Pandas), visualization (Matplotlib, Seaborn), web scraping (BeautifulSoup, Scrapy), and machine learning. Python is widely used for handling large datasets, statistical analysis, and automation.\n",
        "\n",
        "* **R**: A statistical computing language designed for data visualization and analysis, featuring powerful libraries like ggplot2 and Plotly. It supports both structured and unstructured data and is particularly useful for analytics, interactive reporting, and research applications.\n",
        "\n",
        "* **Java**: A platform-independent, object-oriented language used in big data frameworks like Hadoop and Spark. It is well-suited for high-performance, scalable data solutions and large-scale processing tasks.\n",
        "\n",
        "* **Shell Scripting (Unix/Linux Shell, PowerShell)**: Automates repetitive tasks such as file manipulation, system administration, and backups. PowerShell, a Microsoft tool, is particularly effective for handling structured data formats (JSON, CSV, XML) and integrating with APIs.\n",
        "\n",
        "Each language serves a unique role in data management, analysis, and automation, making proficiency in at least one from each category crucial for data professionals."
      ],
      "metadata": {
        "id": "5lcH2NE4SVwn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Repositories\n",
        "\n",
        "A **data repository** is a system used to collect, organize, and store data for business operations, reporting, and analysis. It can range from small to large database infrastructures. Key types of data repositories include:\n",
        "\n",
        "1. **Databases**: Collections of data for storage, search, retrieval, and modification. Managed by a **Database Management System (DBMS)**, which allows users to query and extract information (e.g., finding inactive customers). Databases are classified as:\n",
        "\n",
        "  * **Relational Databases (RDBMS)**: Organize data in tables (rows and columns) with defined structure, optimized for querying large datasets using SQL.\n",
        "\n",
        "  * **Non-Relational Databases (NoSQL)** : Designed for speed, flexibility, and scale, storing data without a strict schema, often used for big data and processing diverse data types.\n",
        "\n",
        "2. **Data Warehouses**: Centralized repositories that consolidate data from various sources through the **ETL (Extract, Transform, Load)** process for business intelligence and analytics. This process helps clean and prepare data for analysis.\n",
        "\n",
        "3. **Data Marts and Data Lakes**: Variants of data repositories, with data lakes storing raw, unprocessed data, and data marts focused on specific business areas.\n",
        "\n",
        "4. **Big Data Stores**: Infrastructure for storing and processing large datasets, using distributed computational and storage systems.\n",
        "\n",
        "Data repositories support efficient reporting, analytics, and archiving, ensuring data is easily accessible and well-organized."
      ],
      "metadata": {
        "id": "eGvrFwoHjM_y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Relational Databases (RDBMS)\n",
        "\n",
        "A **relational database** organizes data into tables with rows (records) and columns (attributes). These tables can be linked based on common data, allowing for efficient querying and retrieval of related information. For example, a **customer table** might be linked to a **transaction table** using a **Customer ID** to consolidate customer data and transactions.\n",
        "\n",
        "Relational databases use **SQL** for querying and processing data, which makes them ideal for large datasets and minimizing redundancy. They allow for data consistency and integrity through structured relationships and data types. Unlike spreadsheets, relational databases support vast volumes of data and provide fast retrieval and processing capabilities.\n",
        "\n",
        "Key benefits include:\n",
        "\n",
        "* **Flexibility**: SQL allows changes to the structure while maintaining performance.\n",
        "\n",
        "* **Reduced redundancy**: Data is stored in linked tables to minimize duplication.\n",
        "\n",
        "* **Backup and recovery**: Easy export/import options and cloud-based continuous mirroring ensure quick recovery.\n",
        "\n",
        "* **ACID compliance**: Ensures reliability and consistency of data transactions.\n",
        "\n",
        "Relational databases are used for **Online Transaction Processing (OLTP), data warehousing**, and **IoT solutions**, though they struggle with **semi-structured** or **unstructured data**. Popular examples include **IBM DB2, MySQL, Oracle Database**, and cloud solutions like **Amazon RDS**.\n",
        "\n",
        "Despite limitations, relational databases remain the primary solution for managing structured data, offering reliability, scalability, and well-established support."
      ],
      "metadata": {
        "id": "xsVumlcqkMAV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NoSQL\n",
        "\n",
        "**NoSQL** (Not Only SQL) is a type of non-relational database that offers flexible schemas for storing and retrieving data. It has become popular due to its scalability, performance, and ease of use, especially in cloud, big data, and high-volume applications. Unlike relational databases, NoSQL databases don't rely on a traditional row/column/table structure with fixed schemas, allowing for schema-less or free-form data storage. They support structured, semi-structured, and unstructured data.\n",
        "\n",
        "There are four main types of NoSQL databases:\n",
        "\n",
        "1. **Key-value store**: Stores data as key-value pairs. It’s efficient for real-time recommendations and caching, but less suited for complex queries. Examples: Redis, DynamoDB.\n",
        "\n",
        "2. **Document-based**: Stores data in documents (e.g., JSON), allowing flexible indexing and powerful ad hoc queries. Best for eCommerce and CRM platforms. Examples: MongoDB, CouchDB.\n",
        "\n",
        "3. **Column-based**: Stores data in columns rather than rows, optimizing fast access and searches. Ideal for systems requiring heavy write requests. Examples: Cassandra, HBase.\n",
        "\n",
        "4. **Graph-based**: Represents data as nodes and relationships, excellent for analyzing connected data (e.g., social networks or fraud detection). Examples: Neo4j, CosmosDB.\n",
        "\n",
        "##### Advantages of NoSQL:\n",
        "\n",
        "* Handles large volumes of diverse data types (structured, semi-structured, unstructured).\n",
        "* Scalable across multiple data centers, leveraging cloud infrastructure.\n",
        "* Cost-effective, with a scale-out architecture that adds capacity and performance by adding nodes.\n",
        "* Agile and flexible design for quicker iteration.\n",
        "\n",
        "##### Key differences between relational and non-relational databases:\n",
        "\n",
        "* **RDBMS**: Rigid schemas, supports ACID-compliance for reliability, and is more expensive to maintain.\n",
        "\n",
        "* **NoSQL**: Schema-agnostic, supports various data types, and is optimized for low-cost hardware and distributed systems.\n",
        "\n",
        "NoSQL is well-suited for modern, mission-critical applications and continues to grow in popularity despite being a newer technology compared to relational databases."
      ],
      "metadata": {
        "id": "FJYKs8yymk7v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Warehouses, Data Marts, and Data Lakes\n",
        "\n",
        "Data mining repositories like **data warehouses**, **data marts**, and **data lakes** all aim to store data for reporting, analysis, and insight extraction, but they differ in purpose, data types, and access methods.\n",
        "\n",
        "* **Data Warehouse**: A central repository integrating data from multiple sources, designed to store cleansed, conformed, and categorized data for analysis. Data warehouses typically have a three-tier architecture (database servers, OLAP server, client front-end) and are moving to the cloud for benefits like lower costs, scalable storage, and faster recovery. They are used for large volumes of operational data, with examples including Teradata and Snowflake.\n",
        "\n",
        "* **Data Mart**: A subset of a data warehouse, focusing on a specific business function (e.g., sales or finance). There are dependent, independent, and hybrid data marts, each varying in data source and transformation processes. Data marts provide efficient access to relevant data and faster decision-making.\n",
        "\n",
        "* **Data Lake**: A repository that stores large amounts of structured, semi-structured, and unstructured data in its raw, native format without needing to define a structure beforehand. It offers flexibility and scalability, allowing data to be transformed as needed for specific use cases. Data lakes can be built using cloud storage or distributed systems like Hadoop. They are beneficial for storing diverse data types and scaling from terabytes to petabytes, with vendors like Amazon and Microsoft providing relevant platforms.\n",
        "\n",
        "Each repository type has specific benefits, and their selection depends on the organization's use case and technology infrastructure."
      ],
      "metadata": {
        "id": "dXd5jkrJpIyz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ETL, ELT, and Data Pipelines\n",
        "\n",
        "ETL and ELT focus on data transformation and loading processes, while data pipelines manage the overall movement and processing of data across systems.\n",
        "\n",
        "1. **ETL (Extract, Transform, Load)**:\n",
        "\n",
        "  * **Extract**: Collect data from source systems using batch (large chunks at scheduled intervals) or stream processing (real-time data).\n",
        "  * **Transform**: Clean and format data for analysis, such as standardizing units, removing duplicates, and applying business rules.\n",
        "  * **Load**: Move the processed data to a repository, with options for initial loading, incremental updates, or full refreshes. Load verification ensures data integrity.\n",
        "  * **ETL** is used for large-scale, batch workloads and is supported by tools like IBM Infosphere, AWS Glue, and Informatica PowerCenter.\n",
        "\n",
        "2. **ELT (Extract, Load, Transform)**:\n",
        "\n",
        "  * In ELT, data is extracted and loaded into the destination system (usually a data lake) before transformations are applied.\n",
        "  * ELT is suited for processing large, unstructured data and offers flexibility for data scientists, especially in Big Data environments. It is ideal for data lakes, allowing quick ingestion of raw data and later transformation for analysis.\n",
        "  * ELT is faster than ETL, reduces cycle time, and supports more flexible analytics.\n",
        "\n",
        "3. **Data Pipelines**:\n",
        "\n",
        "  * Data pipelines refer to the complete journey of moving data between systems, which may include both batch and streaming data.\n",
        "  * Data pipelines support real-time data processing, and destinations include data lakes, applications, or visualization tools. Popular solutions include Apache Beam, AirFlow, and DataFlow."
      ],
      "metadata": {
        "id": "5BcXlFrsrwTP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Integration Platforms: Key Insights\n",
        "### Definition & Purpose\n",
        "Data integration combines disparate data into a unified view for analytics, business operations, and decision-making.\n",
        "\n",
        "It involves:\n",
        "* Extracting, transforming, and merging data\n",
        "* Ensuring data quality and governance\n",
        "* Enabling seamless access and analysis\n",
        "\n",
        "### Relation to ETL & Data Pipelines\n",
        "* **ETL** is a subset of data integration focused on structured transformation.\n",
        "* **Data Pipelines** handle the entire journey of data movement and can include integration processes.\n",
        "\n",
        "### Key Features of Modern Data Integration Platforms\n",
        "* **Pre-built connectors** for databases, APIs, social media, CRM, and ERP systems\n",
        "* **Support for batch and streaming data processing**\n",
        "* **Integration with Big Data sources**\n",
        "* **Cloud flexibility** (single cloud, multi-cloud, hybrid)\n",
        "* **Data governance, security, and compliance features**\n",
        "\n",
        "### Popular Data Integration Tools & Platforms\n",
        "* **Enterprise Solutions**: IBM (Cloud Pak, DataStage), Talend (Data Fabric, Open Studio), SAP, Oracle, Microsoft, Qlik, SAS, TIBCO\n",
        "* **Open-Source Options**: Dell Boomi, Jitterbit, SnapLogic\n",
        "* **Cloud-Based iPaaS (Integration Platform as a Service)**: Google Cloud, IBM Application Integration Suite, Informatica Integration Cloud\n",
        "\n",
        "### Industry Trends\n",
        "Data integration is evolving with new technologies, expanding data sources, and increased demand for real-time analytics. Businesses prioritize scalable, secure, and flexible solutions for effective decision-making.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Key Takeaways on Data Engineering Tools & Technologies\n",
        "### Databases & Storage\n",
        "* Relational Databases (RDBMS): MySQL, PostgreSQL, IBM DB2, Microsoft SQL Server\n",
        "* NoSQL Databases: MongoDB, Cassandra\n",
        "* Graph Databases: Neo4j\n",
        "* Cloud Storage & Data Warehousing: AWS S3 (Data Lake), AWS Redshift (Data Warehouse)\n",
        "\n",
        "### Data Processing & Pipelines\n",
        "* ETL & Data Movement: Talend, Apache NiFi, SSIS (SQL Server Integration Services)\n",
        "* Data Orchestration: Apache Airflow\n",
        "* Big Data Processing: Apache Spark, Hadoop\n",
        "* Streaming & Messaging: Apache Kafka, WebSphere MQ\n",
        "\n",
        "### Automation & Development Tools\n",
        "* Version Control & CI/CD: GitHub, Jenkins\n",
        "* Schema Management: Liquibase\n",
        "* Programming & Scripting: Python (primary language), Shell, Perl, Java APIs\n",
        "\n",
        "### Web Scraping\n",
        "* Tools: BeautifulSoup, Scrapy\n",
        "\n",
        "### Key Advice for Data Engineers\n",
        "* Lifelong Learning: Data engineering is constantly evolving, requiring continuous skill development.\n",
        "* Strong Fundamentals: A solid understanding of data concepts helps in quickly adapting to new tools and technologies.\n",
        "* Open Source Contribution: Exploring and contributing to open-source projects (e.g., Apache Foundation tools) can enhance learning and career growth.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MEa-M4lRL1UN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Foundations of Big Data\n",
        "\n",
        "Big Data refers to the large, dynamic volumes of data generated by people, tools, and machines, which require scalable technology to collect and analyze for real-time insights. The key elements of Big Data are:\n",
        "\n",
        "* Velocity: The speed at which data accumulates and is processed, such as real-time streaming data.\n",
        "* Volume: The scale of data, driven by increased data sources, higher-resolution sensors, and scalable infrastructure.\n",
        "* Variety: The diversity of data, including both structured (e.g., databases) and unstructured data (e.g., social media posts, videos, and images).\n",
        "* Veracity: The quality and accuracy of data, ensuring it is consistent, complete, and reliable.\n",
        "* Value: The ability to derive meaningful insights from data, which can have social, medical, and business benefits.\n",
        "\n",
        "### Examples include:\n",
        "\n",
        "* Velocity: Millions of hours of video uploaded to YouTube every minute.\n",
        "* Volume: 2.5 quintillion bytes of data generated every day by the global population.\n",
        "* Variety: Data from various sources like text, images, health devices, and the Internet of Things.\n",
        "* Veracity: 80% of data being unstructured, requiring careful categorization and analysis.\n",
        "\n",
        "To handle Big Data, tools like Apache Spark and Hadoop are used for distributed computing, allowing businesses to gain valuable insights and improve services."
      ],
      "metadata": {
        "id": "pwxbJRVQozVc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Big Data Processing Tools\n",
        "\n",
        "Big Data processing technologies like Apache Hadoop, Apache Hive, and Apache Spark are essential for analyzing large datasets. Here’s a summary of each:\n",
        "\n",
        "* Hadoop: An open-source framework for distributed storage and processing of large datasets. It uses the Hadoop Distributed File System (HDFS) to store data across multiple nodes, offering scalability, fault tolerance, and parallel computation. It supports structured, semi-structured, and unstructured data and is cost-effective for storing large amounts of data.\n",
        "\n",
        "* Hive: A data warehouse built on top of Hadoop for managing and querying large datasets. It's suited for ETL, reporting, and data analysis but has high query latency, making it less ideal for real-time applications.\n",
        "\n",
        "* Spark: A fast, in-memory data processing engine designed for real-time analytics, machine learning, and data integration. It supports multiple programming languages (Java, Scala, Python, R) and can run on top of Hadoop. Its ability to process streaming data and perform complex analytics makes it a key tool for Big Data analytics.\n",
        "\n",
        "These tools help handle the complexities of Big Data by providing scalable storage, efficient processing, and easy access to insights."
      ],
      "metadata": {
        "id": "2r9oJgq6pbRu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Impact of Big Data on Data Engineering\n",
        "\n",
        "Big Data, characterized by its four Vs—velocity, veracity, volume, and variety—has transformed the field of data engineering, creating both challenges and opportunities. Key points include:\n",
        "\n",
        "* Impact on Data Engineering: The rapid growth of data has created a demand for professionals capable of handling large volumes and diverse types of data. This has led to the emergence of new technologies and tools designed for Big Data management and analysis.\n",
        "\n",
        "* Evolution of Tools: Traditional relational databases (RDBMS) are no longer sufficient for handling the diverse and massive data sets organizations are collecting. New technologies, such as Google BigTable, Cassandra, Hadoop, and MapReduce, were developed to address these needs.\n",
        "\n",
        "* Data Storage and Handling: Storing large amounts of data is no longer a major concern due to advancements in storage technology. However, unstructured data (e.g., from IoT devices or social media) requires specialized solutions like MongoDB.\n",
        "\n",
        "* Growth of Big Data Technologies: As data sources and volumes continue to expand, the role of data engineers has evolved to include managing, processing, and analyzing massive datasets in real-time, utilizing specialized tools and infrastructure.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tS0eeddBphs5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 2 Practice Quiz (1)"
      ],
      "metadata": {
        "id": "A61maGR7HyS7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.) Automated tools, frameworks, and processes for all stages of the data analytics process are part of the Data Engineer’s ecosystem. What role do data integration tools play in this ecosystem?\n",
        "\n",
        "    A. Cover the entire journey of data from source to destination\n",
        "    \n",
        "    B. Store high-volume day-to-day operational data in data repositories\n",
        "    \n",
        "    C. Conduct complex data analytics\n",
        "    \n",
        "    D. Combine data from multiple sources into a unified view that is accessed by data consumers to query and manipulate data\n",
        "\n",
        "  <details>\n",
        "  <summary>ANSWER</summary>\n",
        "    \n",
        "    ↪ D. Combine data from multiple sources into a unified view that is accessed by data consumers to query and manipulate data\n",
        "\n",
        " Data Integration tools provide a unified view to data collected from disparate sources so that it can be accessed via a single interface for query and manipulation by data consumers.\n",
        "</details>\n",
        "\n",
        "2.) Which one of the provided file formats is commonly used by APIs and Web Services to return data?\n",
        "\n",
        "    A. Delimited file\n",
        "    \n",
        "    B. XLS\n",
        "    \n",
        "    C. XML\n",
        "    \n",
        "    D. JSON\n",
        "\n",
        "  <details>\n",
        "  <summary>ANSWER</summary>\n",
        "    \n",
        "    ↪ D. JSON\n",
        "\n",
        " JSON is the format that is most used by APIs and Web Services to return data.\n",
        "</details>\n",
        "\n",
        "3.) What is one example of the relational databases discussed in the video?\n",
        "\n",
        "    A. XML\n",
        "    \n",
        "    B. Spreadsheet\n",
        "    \n",
        "    C. Flat files\n",
        "    \n",
        "    D. SQL Server\n",
        "\n",
        "  <details>\n",
        "  <summary>ANSWER</summary>\n",
        "    \n",
        "    ↪ D. SQL Server\n",
        "\n",
        " SQL Server is one of the examples of relational databases shared in the video.\n",
        "</details>\n",
        "\n",
        "4.) Which of the following languages is one of the most popular querying languages in use today?\n",
        "\n",
        "    A. R\n",
        "    \n",
        "    B. Java\n",
        "    \n",
        "    C. Python\n",
        "    \n",
        "    D. SQL\n",
        "\n",
        "  <details>\n",
        "  <summary>ANSWER</summary>\n",
        "    \n",
        "    ↪ D. SQL\n",
        "\n",
        " SQL, or Structured Query Language, is one of the most popular querying languages in use today.\n",
        "</details>"
      ],
      "metadata": {
        "id": "7kXrWBeHIbx_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 2 Practice Quiz (2)"
      ],
      "metadata": {
        "id": "sbxpUiYcH3CZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.) The term “data repositories” exclusively refers to RDBMes and NoSQL databases that are used to collect, organize, and isolate data for analytics.\n",
        "\n",
        "    A. True\n",
        "    B. False\n",
        "\n",
        "  <details>\n",
        "  <summary>ANSWER</summary>\n",
        "    \n",
        "    ↪ B. False\n",
        "\n",
        " The term “data repositories” includes not just RDBMSes and NoSQL databases, it also includes data warehouses, data marts, and data lakes.\n",
        "</details>\n",
        "\n",
        "2.) In use cases for RDBMS, what is one of the reasons that relational databases are so well suited for OLTP applications?\n",
        "\n",
        "    A. Allow you to make changes in the database even while a query is being executed\n",
        "    B. Offer easy backup and restore options\n",
        "    C. Minimize data redundancy\n",
        "    D. Support the ability to insert, update, or delete small amounts of data\n",
        "\n",
        "  <details>\n",
        "  <summary>ANSWER</summary>\n",
        "    \n",
        "    ↪ D. Support the ability to insert, update, or delete small amounts of data\n",
        "\n",
        " This is one of the abilities of RDBMSs that make them very well suited for OLTP applications.\n",
        "</details>\n",
        "\n",
        "3.) Which NoSQL database type stores each record and its associated data within a single document and also works well with Analytics platforms?\n",
        "\n",
        "    A. Key-value store\n",
        "    B. Column-based\n",
        "    C. Graph-based\n",
        "    D. Document-based\n",
        "\n",
        "  <details>\n",
        "  <summary>ANSWER</summary>\n",
        "    \n",
        "    ↪ D. Document-based\n",
        "\n",
        " Document-based NoSQL databases store each record and its associated data within a single document and work well with Analytics platforms.\n",
        "</details>\n",
        "\n",
        "4.) Which one of these statements explains what data integration is?\n",
        "\n",
        "    A. Data Integration is the process of loading data into a data repository\n",
        "    B. Data Integration is the process of extracting data\n",
        "    C. Data Integration is the process of applying business logic to source data\n",
        "    D. Data Integration includes extracting, transforming, merging, and delivering quality data for analytical purposes\n",
        "\n",
        "  <details>\n",
        "  <summary>ANSWER</summary>\n",
        "    \n",
        "    ↪ D. Data Integration includes extracting, transforming, merging, and delivering quality data for analytical purposes\n",
        "\n",
        " Data Integration extracts and combines disparate source data into a unified view so that data consumers can query and analyze the integrated data.\n",
        "</details>"
      ],
      "metadata": {
        "id": "JwQc0LsGIaZD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 2 Practice Quiz (3)"
      ],
      "metadata": {
        "id": "ELvtFhKkAv4w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.) What does the attribute “Velocity” imply in the context of Big Data?\n",
        "\n",
        "    A. Quality and origin of data\n",
        "    B. Scale of data\n",
        "    C. Diversity of data\n",
        "    D. The speed at which data accumulates\n",
        "\n",
        "  <details>\n",
        "  <summary>ANSWER</summary>\n",
        "    \n",
        "    ↪ D. The speed at which data accumulates\n",
        "\n",
        " Velocity, in the context of Big Data, is the speed at which data accumulates.\n",
        "</details>\n",
        "\n",
        "2.) Which of the Big Data processing tools provides distributed storage and processing of Big Data?\n",
        "\n",
        "    A. ETL\n",
        "    B. Spark\n",
        "    C. Hadoop\n",
        "    D. Hive\n",
        "\n",
        "  <details>\n",
        "  <summary>ANSWER</summary>\n",
        "    \n",
        "    ↪ C. Hadoop\n",
        "\n",
        " Hadoop, a java-based open-source framework, allows distributed storage and processing of large datasets across clusters of computers.\n",
        "</details>"
      ],
      "metadata": {
        "id": "iVFVHen6Axmx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 2 Graded Quiz"
      ],
      "metadata": {
        "id": "ZfhUn6FRH-IF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.) There are two main types of data repositories – Transactional and Analytical. For high-volume day-to-day operational data such as banking transactions, Transactional, or OLTP, systems are the ideal choice.\n",
        "\n",
        "    A. TRUE  \n",
        "    B. FALSE\n",
        "\n",
        "<details>\n",
        "  <summary>ANSWER</summary>\n",
        "  ↪ A. TRUE\n",
        "  \n",
        "Transactional, or OLTP, systems are designed and optimized for handling high-volume transactions.\n",
        "</details>\n",
        "\n",
        "2.) Which of the following is an example of unstructured data?\n",
        "\n",
        "    A. Spreadsheets  \n",
        "    B. Zipped files  \n",
        "    C. Video and Audio files  \n",
        "    D. XML  \n",
        "\n",
        "<details>\n",
        "  <summary>ANSWER</summary>\n",
        "  ↪ C. Video and Audio files\n",
        "\n",
        "  Video and audio files are examples of unstructured data.\n",
        "</details>\n",
        "\n",
        "3.) Which one of these file formats is independent of software, hardware, and operating systems, and can be viewed the same way on any device?  \n",
        "\n",
        "    A. Delimited text file  \n",
        "    B. XLSX  \n",
        "    C. PDF  \n",
        "    D. XML  \n",
        "\n",
        "<details>\n",
        "  <summary>ANSWER</summary>\n",
        "  ↪ C. PDF\n",
        "\n",
        "  PDF format is independent of software, hardware, and operating systems, and can be viewed the same way on any device.\n",
        "</details>\n",
        "\n",
        "4.) Which data source can return data in plain text, XML, HTML, or JSON among others?  \n",
        "\n",
        "    A. Delimited text file  \n",
        "    B. PDF  \n",
        "    C. APIs  \n",
        "    D. XML  \n",
        "\n",
        "<details>\n",
        "  <summary>ANSWER</summary>\n",
        "  ↪ C. APIs\n",
        "\n",
        "  APIs can return data in a wide variety of formats such as plain text, XML, HTML, or JSON among others.\n",
        "</details>\n",
        "\n",
        "5.) In the data engineer’s ecosystem, languages are classified by type. What are shell and scripting languages most commonly used for?\n",
        "\n",
        "    A. Manipulating data  \n",
        "    B. Querying data  \n",
        "    C. Automating repetitive operational tasks  \n",
        "    D. Building apps  \n",
        "\n",
        "<details>\n",
        "  <summary>ANSWER</summary>\n",
        "  ↪ C. Automating repetitive operational tasks\n",
        "\n",
        "  Shell and scripting languages are commonly used for automating repetitive operational tasks.\n",
        "</details>\n",
        "\n",
        "6.) What is one of the most significant advantages of an RDBMS?  \n",
        "\n",
        "    A. Enforces a limit on the length of data fields  \n",
        "    B. Can store only structured data  \n",
        "    C. Is ACID-Compliant  \n",
        "    D. Requires source and destination tables to be identical for migrating data  \n",
        "\n",
        "<details>\n",
        "  <summary>ANSWER</summary>\n",
        "  ↪ C. Is ACID-Compliant  \n",
        "  \n",
        "  ACID-Compliance is one of the significant advantages of an RDBMS.\n",
        "</details>\n",
        "\n",
        "7.) Which one of the NoSQL database types uses a graphical model to represent and store data, and is particularly useful for visualizing, analyzing, and finding connections between different pieces of data?\n",
        "\n",
        "    A. Document-based  \n",
        "    B. Column-based  \n",
        "    C. Graph-based  \n",
        "    D. Key value store  \n",
        "\n",
        "<details>\n",
        "  <summary>ANSWER</summary>\n",
        "  ↪ C. Graph-based  \n",
        "  \n",
        "  Graph-based NoSQL databases use a graphical model to represent and store data and are used for visualizing, analyzing, and finding connections between different pieces of data.\n",
        "</details>\n",
        "\n",
        "8.) Which of the data repositories serves as a pool of raw data and stores large amounts of structured, semi-structured, and unstructured data in their native formats?\n",
        "\n",
        "    A. Relational Databases  \n",
        "    B. Data Marts  \n",
        "    C. Data Lakes  \n",
        "    D. Data Warehouses  \n",
        "\n",
        "<details>\n",
        "  <summary>ANSWER</summary>\n",
        "  ↪ C. Data Lakes  \n",
        "  \n",
        "  A Data Lake can store large amounts of structured, semi-structured, and unstructured data in their native format, classified and tagged with metadata.\n",
        "</details>\n",
        "\n",
        "9.) While data integration combines disparate data into a unified view of the data, a data pipeline covers the entire data movement journey from source to destination systems, and ETL is a process within data integration.\n",
        "\n",
        "    A. TRUE  \n",
        "    B. FALSE  \n",
        "\n",
        "<details>\n",
        "  <summary>ANSWER</summary>\n",
        "  ↪ A. TRUE  \n",
        "  \n",
        "  A data pipeline covers the entire journey of data from source to destination. Data integration is performed within a data pipeline, while ETL is a process within data integration.\n",
        "</details>\n",
        "\n",
        "10.) What does the attribute “Veracity” imply in the context of Big Data?\n",
        "\n",
        "    A. The speed at which data accumulates  \n",
        "    B. Scale of data  \n",
        "    C. Diversity of the type and sources of data  \n",
        "    D. Accuracy and conformity of data to facts  \n",
        "\n",
        "<details>\n",
        "  <summary>ANSWER</summary>\n",
        "  ↪ D. Accuracy and conformity of data to facts  \n",
        "  \n",
        "  Veracity, in the context of Big Data, refers to the accuracy and conformity of data to facts.\n",
        "</details>\n",
        "\n",
        "11.) ______________, in the context of Big Data, is the speed at which data accumulates.\n",
        "\n",
        "    A. Value  \n",
        "    B. Volume  \n",
        "    C. Variety  \n",
        "    D. Velocity  \n",
        "\n",
        "<details>\n",
        "  <summary>ANSWER</summary>\n",
        "  ↪ D. Velocity  \n",
        "  \n",
        "  Velocity refers to the speed at which data is generated, such as, real-time streaming data.\n",
        "</details>\n",
        "\n",
        "12.) Apache Spark is a general-purpose data processing engine designed to extract and process Big Data for a wide range of applications. What is one of its key use cases?\n",
        "\n",
        "    A. Consolidate data across the organization  \n",
        "    B. Scalable and reliable Big Data storage  \n",
        "    C. Fast recovery from hardware failures  \n",
        "    D. Perform complex analytics in real-time  \n",
        "\n",
        "<details>\n",
        "  <summary>ANSWER</summary>\n",
        "  ↪ D. Perform complex analytics in real-time  \n",
        "  \n",
        "  Spark is a general-purpose data processing engine used for performing complex data analytics in real-time.\n",
        "</details>\n",
        "\n",
        "13.) Which of the Big Data processing tools is used for reading, writing, and managing large data set files that are stored in either HDFS or Apache HBase?\n",
        "\n",
        "    A. ETL  \n",
        "    B. Spark  \n",
        "    C. Hadoop  \n",
        "    D. Hive  \n",
        "\n",
        "<details>\n",
        "  <summary>ANSWER</summary>\n",
        "  ↪ D. Hive  \n",
        "  \n",
        "  Hive is an open-source data warehouse software for reading, writing, and managing large data sets stored in data storage systems such as HDFS and Apache HBase.\n",
        "</details>"
      ],
      "metadata": {
        "id": "Nn87vhm0IQVZ"
      }
    }
  ]
}